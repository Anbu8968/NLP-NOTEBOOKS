{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai as it costs only 2$ per plate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "Strange\n",
      "loves\n",
      "pav\n",
      "bhaji\n",
      "of\n",
      "mumbai\n",
      "as\n",
      "it\n",
      "costs\n",
      "only\n",
      "2\n",
      "$\n",
      "per\n",
      "plate\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dr."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Strange'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "span=doc[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dr. Strange loves pav bhaji"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(\"Stanly gave two $ to his brother\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "token0=doc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stanly"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0.is_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0.like_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# collecting email id from sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stanly@divum.in \\n',\n",
       " 'anbu@divum.in\\n',\n",
       " 'darshan@divum.in\\n',\n",
       " 'vijesh@divum.in']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"students.txt\") as f:\n",
    "    text=f.readlines()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stanly@divum.in \\n anbu@divum.in\\n darshan@divum.in\\n vijesh@divum.in'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\" \".join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stanly@divum.in \n",
       " anbu@divum.in\n",
       " darshan@divum.in\n",
       " vijesh@divum.in"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails=[]\n",
    "for i in doc:\n",
    "    if i.like_email:\n",
    "        emails.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stanly@divum.in, anbu@divum.in, darshan@divum.in, vijesh@divum.in]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# customizing token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gimme', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp=spacy.blank('en')\n",
    "doc=nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "tokens=[token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gim', 'me', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.tokenizer.add_special_case('gimme',[\n",
    "    {ORTH:\"gim\"},\n",
    "    {ORTH:\"me\"}])\n",
    "doc=nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "tokens=[token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Tokenization or Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24263/1994801932.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36msents\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`."
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\")\n",
    "for s in doc.sents:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x7fe4b1bbef00>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7fe4b1bbef00>)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Strange loves pav bhaji of mumbai.\n",
      "Hulk loves chat of delhi\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\")\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research is often freely available. Good\n",
    "places to start include http://www.data.gov/, and http://www.science.\n",
    "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
    "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
    "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.data.gov/\n",
      "http://www.science\n",
      "http://data.gov.uk/.\n",
      "http://www3.norc.org/gss+website/\n",
      "http://www.europeansocialsurvey.org/.\n"
     ]
    }
   ],
   "source": [
    "url=[]\n",
    "for i in doc:\n",
    "    if i.like_url:\n",
    "        url.append(i)\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[http://www.data.gov/,\n",
       " http://www.science,\n",
       " http://data.gov.uk/.,\n",
       " http://www3.norc.org/gss+website/,\n",
       " http://www.europeansocialsurvey.org/.]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two $\n",
      "500 €\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(transactions)\n",
    "for token in doc:\n",
    "    if token.like_num and doc[token.i+1].is_currency:\n",
    "        print(token.text,doc[token.i+1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipeline tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Blank NLP pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain\n",
      "america\n",
      "ate\n",
      "100\n",
      "$\n",
      "of\n",
      "samosa\n",
      ".\n",
      "Then\n",
      "he\n",
      "said\n",
      "I\n",
      "can\n",
      "do\n",
      "this\n",
      "all\n",
      "day\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "nlp=spacy.blank(\"en\")\n",
    "doc=nlp(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")\n",
    "for t in doc:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7fe571a4eb60>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7fe54cf3efe0>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7fe54cd41770>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7fe54c756cc0>),\n",
       " ('lemmatizer',\n",
       "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7fe54c77f500>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7fe54cc4d230>)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain  |  proper noun  |  Captain\n",
      "america  |  proper noun  |  america\n",
      "ate  |  verb  |  eat\n",
      "100  |  numeral  |  100\n",
      "$  |  noun  |  $\n",
      "of  |  adposition  |  of\n",
      "samosa  |  proper noun  |  samosa\n",
      ".  |  punctuation  |  .\n",
      "Then  |  adverb  |  then\n",
      "he  |  pronoun  |  he\n",
      "said  |  verb  |  say\n",
      "I  |  pronoun  |  I\n",
      "can  |  auxiliary  |  can\n",
      "do  |  verb  |  do\n",
      "this  |  pronoun  |  this\n",
      "all  |  determiner  |  all\n",
      "day  |  noun  |  day\n",
      ".  |  punctuation  |  .\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")\n",
    "for token in doc:\n",
    "    print(token,\" | \",spacy.explain(token.pos_),\" | \",token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Entity Recognition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  Companies, agencies, institutions, etc.\n",
      "$45 billion  |  Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc= nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "for e in doc.ents:\n",
    "    print(e.text,\" | \",spacy.explain(e.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to acquire twitter for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $45 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc,style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a component to a blank pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ner']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"ner\", source=source_nlp)\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc ORG\n",
      "$45 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Language Processing Pipelines: Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp =spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" Ravi and Raju are the best friends from school days.They wanted to go for a world tour and visit famous cities like Paris, London, Dubai, Rome etc and also they called their another friend Mohan to take part of this world tour.They started their journey from Hyderabad and spent next 3 months travelling all the wonderful cities in the world and cherish a happy moments!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ravi, Raju, Paris, London, Dubai, Rome, Mohan, Hyderabad] 8\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(text)\n",
    "names=[]\n",
    "for i in doc:\n",
    "#     print(i.pos_,\" | \",i)\n",
    "    if i.pos_==\"PROPN\":\n",
    "        names.append(i)\n",
    "print(names,len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''The Top 5 companies in USA are Tesla, Walmart, Amazon, Microsoft, Google and the top 5 companies in \n",
    "India are Infosys, Reliance, HDFC Bank, Hindustan Unilever and Bharti Airtel'''\n",
    "\n",
    "\n",
    "doc = nlp(text)\n",
    "# doc=\" \".join(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_names=[]\n",
    "for i in doc.ents:\n",
    "    if i.label_==\"ORG\":\n",
    "        c_names.append(i)\n",
    "company_names=[]\n",
    "for i in c_names:\n",
    "    x=str(i).split(',')\n",
    "    for j in x:\n",
    "        company_names.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(company_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tesla', 'Walmart', 'Amazon', 'Microsoft', 'Google', 'Infosys', ' Reliance', 'HDFC Bank', 'Hindustan Unilever', 'Bharti Airtel']\n"
     ]
    }
   ],
   "source": [
    "print(company_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "$45 billion  |  MONEY  |  Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,\" | \",ent.label_,\" | \",spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to acquire twitter for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $45 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc,style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List down all the entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CARDINAL',\n",
       " 'DATE',\n",
       " 'EVENT',\n",
       " 'FAC',\n",
       " 'GPE',\n",
       " 'LANGUAGE',\n",
       " 'LAW',\n",
       " 'LOC',\n",
       " 'MONEY',\n",
       " 'NORP',\n",
       " 'ORDINAL',\n",
       " 'ORG',\n",
       " 'PERCENT',\n",
       " 'PERSON',\n",
       " 'PRODUCT',\n",
       " 'QUANTITY',\n",
       " 'TIME',\n",
       " 'WORK_OF_ART']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_labels['ner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael Bloomberg | PERSON | People, including fictional\n",
      "Bloomberg | PERSON | People, including fictional\n",
      "1982 | DATE | Absolute or relative dates or periods\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Michael Bloomberg founded Bloomberg in 1982\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"|\", ent.label_, \"|\", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  ORG  |  0 | 9\n",
      "Twitter Inc  |  ORG  |  30 | 41\n",
      "$45 billion  |  MONEY  |  46 | 57\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc = nlp(\"Tesla Inc is going to acquire Twitter Inc for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_, \" | \", ent.start_char, \"|\", ent.end_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Custom Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter  |  ORG\n",
      "$45 billion  |  MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla is going to acquire Twitter for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "going to acquire"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = doc[2:5]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "s1=Span(doc,0,1,label='ORG')\n",
    "s2=Span(doc,5,6,label='ORG')\n",
    "doc.set_ents([s1,s2],default=\"unmodified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla  |  ORG\n",
      "Twitter  |  ORG\n",
      "$45 billion  |  MONEY\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text,\" | \",ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER Excercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Kiran want to know the famous foods in each state of India. So, he opened Google and search for this question. Google showed that\n",
    "in Delhi it is Chaat, in Gujarat it is Dal Dhokli, in Pongal it is Tamilnadu, in Andhrapradesh it is Biryani, in Assam it is Papaya Khar,\n",
    "in Bihar it is Litti Chowkha and so on for all other states\"\"\"\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_names=[]\n",
    "for ent in doc.ents:\n",
    "    if ent.label_==\"GPE\":\n",
    "        loc_names.append(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kiran\n",
      "India\n",
      "Delhi\n",
      "Gujarat\n",
      "Pongal\n",
      "Andhrapradesh\n",
      "Bihar\n"
     ]
    }
   ],
   "source": [
    "for i in loc_names:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geographical location Names:  [Kiran, India, Delhi, Gujarat, Pongal, Andhrapradesh, Bihar]\n",
      "Count:  7\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Kiran want to know the famous foods in each state of India. So, he opened Google and search for this question. Google showed that\n",
    "in Delhi it is Chaat, in Gujarat it is Dal Dhokli, in Tamilnadu it is Pongal, in Andhrapradesh it is Biryani, in Assam it is Papaya Khar,\n",
    "in Bihar it is Litti Chowkha and so on for all other states\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "\n",
    "\n",
    "#list for storing all the names\n",
    "all_gpe_names = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'GPE':     #checking the whether token belongs to entity \"GPE\" [Geographical location]\n",
    "        all_gpe_names.append(ent)\n",
    "\n",
    "\n",
    "\n",
    "#finally printing the results\n",
    "print(\"Geographical location Names: \", all_gpe_names)\n",
    "print(\"Count: \", len(all_gpe_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Birth Dates:  [24 April 1973, 5 November 1988, 7 July 1981, 19 December 1974]\n",
      "Count:  4\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Sachin Tendulkar was born on 24 April 1973, Virat Kholi was born on 5 November 1988, Dhoni was born on 7 July 1981\n",
    "and finally Ricky ponting was born on 19 December 1974.\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "\n",
    "\n",
    "#list for storing all the dates\n",
    "all_birth_dates = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'DATE':     #checking the whether token belongs to entity \"DATE\" [Dates]\n",
    "        all_birth_dates.append(ent)\n",
    "\n",
    "\n",
    "\n",
    "#finally printing the results\n",
    "print(\"All Birth Dates: \", all_birth_dates)\n",
    "print(\"Count: \", len(all_birth_dates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Tutorial : Text Representation - Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"spam.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['spam']=df[\"Category\"].apply(lambda x: 1 if x=='spam' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message  spam\n",
       "0      ham  Go until jurong point, crazy.. Available only ...     0\n",
       "1      ham                      Ok lar... Joking wif u oni...     0\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...     1\n",
       "3      ham  U dun say so early hor... U c already then say...     0\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...     0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(df.Message,df.spam,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4457,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1115,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3377    0\n",
       "3401    1\n",
       "3918    0\n",
       "5303    0\n",
       "Name: spam, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create bag of words representation using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4457x7755 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 59395 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "v=CountVectorizer()\n",
    "X_train_cv = v.fit_transform(X_train.values)\n",
    "X_train_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cv.toarray()[:2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4457, 7755)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '000pes', ..., 'zouk', 'zyada', 'ú1'], dtype=object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4457, 7755)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chikku'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.get_feature_names_out()[1778]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'good': 3190,\n",
       " 'afternon': 846,\n",
       " 'my': 4667,\n",
       " 'love': 4234,\n",
       " 'how': 3525,\n",
       " 'are': 1048,\n",
       " 'today': 6940,\n",
       " 'hope': 3493,\n",
       " 'your': 7721,\n",
       " 'and': 958,\n",
       " 'maybe': 4404,\n",
       " 'have': 3362,\n",
       " 'some': 6300,\n",
       " 'interviews': 3714,\n",
       " 'wake': 7360,\n",
       " 'miss': 4513,\n",
       " 'you': 7715,\n",
       " 'babe': 1185,\n",
       " 'passionate': 5107,\n",
       " 'kiss': 3952,\n",
       " 'from': 3021,\n",
       " 'across': 786,\n",
       " 'the': 6817,\n",
       " 'sea': 5974,\n",
       " 'as': 1083,\n",
       " 'valued': 7251,\n",
       " 'customer': 2121,\n",
       " 'am': 929,\n",
       " 'pleased': 5254,\n",
       " 'to': 6934,\n",
       " 'advise': 829,\n",
       " 'that': 6813,\n",
       " 'following': 2926,\n",
       " 'recent': 5636,\n",
       " 'review': 5776,\n",
       " 'of': 4894,\n",
       " 'mob': 4545,\n",
       " 'no': 4793,\n",
       " 'awarded': 1168,\n",
       " 'with': 7556,\n",
       " '1500': 294,\n",
       " 'bonus': 1417,\n",
       " 'prize': 5429,\n",
       " 'call': 1600,\n",
       " '09066364589': 221,\n",
       " 'eh': 2544,\n",
       " 'ur': 7204,\n",
       " 'laptop': 4020,\n",
       " 'got': 3207,\n",
       " 'stock': 6488,\n",
       " 'lei': 4077,\n",
       " 'he': 3373,\n",
       " 'say': 5936,\n",
       " 'mon': 4570,\n",
       " 'muz': 4665,\n",
       " 'come': 1905,\n",
       " 'again': 852,\n",
       " 'take': 6688,\n",
       " 'look': 4201,\n",
       " 'not': 4828,\n",
       " 'can': 1623,\n",
       " 'but': 1569,\n",
       " 'it': 3758,\n",
       " 'will': 7526,\n",
       " 'tell': 6756,\n",
       " 'quite': 5540,\n",
       " 'long': 4197,\n",
       " 'cos': 2010,\n",
       " 'haven': 3364,\n",
       " 'finish': 2862,\n",
       " 'film': 2848,\n",
       " 'yet': 7702,\n",
       " 'sorry': 6335,\n",
       " 'went': 7464,\n",
       " 'bed': 1287,\n",
       " 'early': 2501,\n",
       " 'nightnight': 4776,\n",
       " 'dear': 2185,\n",
       " 'tmorrow': 6927,\n",
       " 'pls': 5260,\n",
       " 'accomodate': 771,\n",
       " 'lol': 4190,\n",
       " 'now': 4840,\n",
       " 'after': 845,\n",
       " 'hot': 3511,\n",
       " 'air': 878,\n",
       " 'balloon': 1213,\n",
       " 'reach': 5598,\n",
       " 'home': 3477,\n",
       " 'in': 3644,\n",
       " 'lt': 4258,\n",
       " 'gt': 3264,\n",
       " 'minutes': 4504,\n",
       " 'be': 1273,\n",
       " 'office': 4903,\n",
       " 'around': 1070,\n",
       " 'pm': 5267,\n",
       " 'going': 3179,\n",
       " 'hospital': 3507,\n",
       " 'mail': 4327,\n",
       " 'so': 6286,\n",
       " 'what': 7481,\n",
       " 'about': 748,\n",
       " 'do': 2371,\n",
       " 'remember': 5695,\n",
       " 'we': 7419,\n",
       " 'make': 4334,\n",
       " 'baby': 1188,\n",
       " 'yo': 7710,\n",
       " 'tho': 6862,\n",
       " 'might': 4478,\n",
       " 'go': 3166,\n",
       " 'sch': 5949,\n",
       " 'yar': 7680,\n",
       " 'at': 1117,\n",
       " 'salon': 5896,\n",
       " 'boring': 1432,\n",
       " 'ready': 5606,\n",
       " 'garbage': 3083,\n",
       " 'bags': 1202,\n",
       " 'eggs': 2542,\n",
       " 'jam': 3786,\n",
       " 'bread': 1482,\n",
       " 'hannaford': 3326,\n",
       " 'wheat': 7485,\n",
       " 'chex': 1768,\n",
       " 'free': 2987,\n",
       " 'entry': 2616,\n",
       " 'wkly': 7569,\n",
       " 'comp': 1919,\n",
       " 'win': 7529,\n",
       " 'fa': 2740,\n",
       " 'cup': 2108,\n",
       " 'final': 2853,\n",
       " 'tkts': 6921,\n",
       " '21st': 348,\n",
       " 'may': 4402,\n",
       " '2005': 339,\n",
       " 'text': 6790,\n",
       " '87121': 678,\n",
       " 'receive': 5632,\n",
       " 'question': 5533,\n",
       " 'std': 6466,\n",
       " 'txt': 7095,\n",
       " 'rate': 5583,\n",
       " 'apply': 1024,\n",
       " '08452810075over18': 64,\n",
       " 'evening': 2662,\n",
       " 'this': 6855,\n",
       " 'is': 3746,\n",
       " 'roger': 5821,\n",
       " 'brother': 1517,\n",
       " 'genius': 3113,\n",
       " 'freemsg': 2993,\n",
       " '86888': 673,\n",
       " 'claim': 1820,\n",
       " 'reward': 5778,\n",
       " 'hours': 3519,\n",
       " 'talk': 6696,\n",
       " 'time': 6901,\n",
       " 'use': 7221,\n",
       " 'phone': 5194,\n",
       " 'subscribe6gbp': 6561,\n",
       " 'mnth': 4541,\n",
       " 'inc': 3646,\n",
       " '3hrs': 454,\n",
       " '16': 307,\n",
       " 'stop': 6498,\n",
       " 'txtstop': 7105,\n",
       " 'though': 6866,\n",
       " 'shd': 6084,\n",
       " 'out': 5013,\n",
       " 'fun': 3045,\n",
       " 'bar': 1224,\n",
       " 'town': 7005,\n",
       " 'or': 4979,\n",
       " 'something': 6309,\n",
       " 'sound': 6343,\n",
       " 'ok': 4918,\n",
       " 'horrible': 3505,\n",
       " 'gal': 3069,\n",
       " 'knew': 3961,\n",
       " 'dat': 2166,\n",
       " 'was': 7395,\n",
       " 'wif': 7516,\n",
       " 'him': 3438,\n",
       " 'yest': 7700,\n",
       " 'still': 6485,\n",
       " 'ask': 1092,\n",
       " 'me': 4411,\n",
       " 'word': 7599,\n",
       " 'checkmate': 1753,\n",
       " 'chess': 1766,\n",
       " 'comes': 1907,\n",
       " 'persian': 5168,\n",
       " 'phrase': 5202,\n",
       " 'shah': 6066,\n",
       " 'maat': 4299,\n",
       " 'which': 7496,\n",
       " 'means': 4418,\n",
       " 'king': 3946,\n",
       " 'dead': 2181,\n",
       " 'goodmorning': 3195,\n",
       " 'day': 2175,\n",
       " 'month': 4580,\n",
       " 'kotees': 3978,\n",
       " 'birthday': 1363,\n",
       " 'know': 3965,\n",
       " 'meet': 4429,\n",
       " 'lunch': 4270,\n",
       " 'la': 3990,\n",
       " 'picking': 5208,\n",
       " 'them': 6825,\n",
       " 'up': 7187,\n",
       " 'various': 7256,\n",
       " 'points': 5287,\n",
       " 'yeovil': 7696,\n",
       " 'they': 6841,\n",
       " 'motor': 4602,\n",
       " 'project': 5452,\n",
       " 'then': 6829,\n",
       " '12': 270,\n",
       " '30': 417,\n",
       " 'max': 4398,\n",
       " 'very': 7279,\n",
       " 'easy': 2512,\n",
       " 'search': 5975,\n",
       " 'happiness': 3337,\n",
       " 'main': 4331,\n",
       " 'sources': 6349,\n",
       " 'unhappiness': 7157,\n",
       " 'accept': 764,\n",
       " 'life': 4106,\n",
       " 'way': 7416,\n",
       " 'find': 2858,\n",
       " 'every': 2668,\n",
       " 'moment': 4567,\n",
       " 'live': 4157,\n",
       " 'ill': 3616,\n",
       " '2mrw': 396,\n",
       " 'ninish': 4783,\n",
       " 'address': 807,\n",
       " 'icky': 3588,\n",
       " 'american': 934,\n",
       " 'freek': 2991,\n",
       " 'wont': 7592,\n",
       " 'callin': 1611,\n",
       " 'bad': 1197,\n",
       " 'jen': 3813,\n",
       " 'alrite': 921,\n",
       " 'sam': 5898,\n",
       " 'its': 3766,\n",
       " 'nic': 4766,\n",
       " 'just': 3878,\n",
       " 'checkin': 1751,\n",
       " 'number': 4855,\n",
       " 'taste': 6712,\n",
       " 'fish': 2874,\n",
       " 'curry': 2115,\n",
       " 'hello': 3404,\n",
       " 'drivby': 2452,\n",
       " '0quit': 244,\n",
       " 'edrunk': 2526,\n",
       " 'iff': 3605,\n",
       " 'pthis': 5490,\n",
       " 'makes': 4335,\n",
       " 'senrd': 6024,\n",
       " 'dnot': 2369,\n",
       " 'dancce': 2146,\n",
       " 'drum': 2466,\n",
       " 'basq': 1243,\n",
       " 'ihave': 3611,\n",
       " '2nhite': 399,\n",
       " 'ros': 5833,\n",
       " 'xxxxxxx': 7671,\n",
       " 'hurt': 3563,\n",
       " 'see': 5992,\n",
       " 'letter': 4092,\n",
       " 'on': 4937,\n",
       " 'car': 1641,\n",
       " 'bugis': 1546,\n",
       " 'juz': 3883,\n",
       " 'wat': 7402,\n",
       " 'walking': 7367,\n",
       " 'oredi': 4989,\n",
       " 'late': 4029,\n",
       " 'reply': 5727,\n",
       " 'oso': 5002,\n",
       " 'saw': 5935,\n",
       " 'top': 6982,\n",
       " 'like': 4117,\n",
       " 'din': 2320,\n",
       " 'buy': 1572,\n",
       " 'where': 7491,\n",
       " 'cool': 1996,\n",
       " 'when': 7487,\n",
       " 'head': 3374,\n",
       " 'bank': 1220,\n",
       " 'granite': 3230,\n",
       " 'issues': 3756,\n",
       " 'strong': 6529,\n",
       " 'explosive': 2725,\n",
       " 'pick': 5206,\n",
       " 'for': 2942,\n",
       " 'our': 5011,\n",
       " 'members': 4445,\n",
       " 'over': 5030,\n",
       " '300': 418,\n",
       " 'nasdaq': 4698,\n",
       " 'symbol': 6666,\n",
       " 'cdgt': 1690,\n",
       " '00': 0,\n",
       " 'per': 5153,\n",
       " 'twenty': 7086,\n",
       " 'past': 5111,\n",
       " 'five': 2878,\n",
       " 'said': 5888,\n",
       " 'train': 7013,\n",
       " 'been': 1293,\n",
       " 'durham': 2487,\n",
       " 'already': 919,\n",
       " 'coz': 2039,\n",
       " 'reserved': 5744,\n",
       " 'seat': 5978,\n",
       " 'hey': 3428,\n",
       " 'there': 6834,\n",
       " 'leave': 4065,\n",
       " 'friday': 3005,\n",
       " 'wait': 7356,\n",
       " 'superior': 6614,\n",
       " 'well': 7460,\n",
       " 'give': 3151,\n",
       " 'all': 909,\n",
       " 'didn': 2297,\n",
       " 'one': 4943,\n",
       " 'nighters': 4775,\n",
       " 'persevered': 5167,\n",
       " 'found': 2968,\n",
       " 'cheap': 1743,\n",
       " 'apologise': 1014,\n",
       " 'advance': 826,\n",
       " 'somewhere': 6313,\n",
       " 'sleep': 6220,\n",
       " 'isnt': 3754,\n",
       " 'did': 2295,\n",
       " 'gonna': 3189,\n",
       " 'die': 2300,\n",
       " 'stay': 6461,\n",
       " 'here': 3418,\n",
       " 'thinkin': 6848,\n",
       " 'goin': 3177,\n",
       " 'needs': 4730,\n",
       " 'fucking': 3034,\n",
       " 'dealing': 2184,\n",
       " 'hmm': 3457,\n",
       " 'thinking': 6849,\n",
       " 'lor': 4212,\n",
       " 'sent': 6029,\n",
       " 'an': 953,\n",
       " 'email': 2569,\n",
       " 'incomm': 3655,\n",
       " 'right': 5790,\n",
       " 'chance': 1715,\n",
       " '250': 359,\n",
       " 'cash': 1667,\n",
       " 'wk': 7565,\n",
       " 'play': 5244,\n",
       " '83370': 654,\n",
       " 'www': 7650,\n",
       " 'music': 4658,\n",
       " 'trivia': 7045,\n",
       " 'net': 4742,\n",
       " 'custcare': 2119,\n",
       " '08715705022': 120,\n",
       " '1x150p': 332,\n",
       " 'afternoon': 847,\n",
       " 'words': 7600,\n",
       " 'ym': 7709,\n",
       " 'get': 3123,\n",
       " 'tm': 6924,\n",
       " 'smart': 6246,\n",
       " 'move': 4607,\n",
       " 'slave': 6219,\n",
       " 'smiles': 6255,\n",
       " 'drink': 2448,\n",
       " 'coffee': 1878,\n",
       " 'await': 1164,\n",
       " 'uniform': 7161,\n",
       " 'urgent': 7208,\n",
       " 'mobile': 4547,\n",
       " 'has': 3348,\n",
       " '2000': 336,\n",
       " 'guaranteed': 3266,\n",
       " '09058094455': 166,\n",
       " 'land': 4007,\n",
       " 'line': 4132,\n",
       " '3030': 422,\n",
       " 'valid': 7247,\n",
       " '12hrs': 279,\n",
       " 'only': 4948,\n",
       " 'should': 6126,\n",
       " 'made': 4312,\n",
       " 'appointment': 1027,\n",
       " 'want': 7381,\n",
       " 'custom': 2120,\n",
       " 'officer': 4904,\n",
       " 'discount': 2344,\n",
       " 'oh': 4913,\n",
       " 'yes': 7699,\n",
       " 'why': 7510,\n",
       " 'texted': 6796,\n",
       " 'pshew': 5484,\n",
       " 'missing': 4516,\n",
       " 'much': 4636,\n",
       " 'tonight': 6968,\n",
       " 'mate': 4385,\n",
       " 'catching': 1678,\n",
       " 'new': 4757,\n",
       " 'by': 1584,\n",
       " 'll': 4164,\n",
       " 'hand': 3315,\n",
       " 'her': 3417,\n",
       " 'chat': 1738,\n",
       " 'wit': 7555,\n",
       " 'request': 5734,\n",
       " 'melle': 4440,\n",
       " 'oru': 4999,\n",
       " 'minnaminunginte': 4499,\n",
       " 'nurungu': 4858,\n",
       " 'vettam': 7280,\n",
       " 'set': 6045,\n",
       " 'callertune': 1609,\n",
       " 'callers': 1608,\n",
       " 'press': 5401,\n",
       " 'copy': 2001,\n",
       " 'friends': 3010,\n",
       " 'notice': 4834,\n",
       " 'looking': 4205,\n",
       " 'shit': 6107,\n",
       " 'mirror': 4507,\n",
       " 'youre': 7722,\n",
       " 'turning': 7080,\n",
       " 'into': 3717,\n",
       " 'freak': 2982,\n",
       " 'responding': 5755,\n",
       " 'fancy': 2772,\n",
       " 'flirt': 2899,\n",
       " 'date': 2168,\n",
       " 'join': 3840,\n",
       " 'uks': 7125,\n",
       " 'fastest': 2783,\n",
       " 'growing': 3258,\n",
       " 'dating': 2171,\n",
       " 'service': 6041,\n",
       " 'msgs': 4625,\n",
       " 'rcvd': 5594,\n",
       " '25p': 362,\n",
       " 'optout': 4978,\n",
       " '83021': 646,\n",
       " 'actually': 798,\n",
       " 'sleeping': 6222,\n",
       " 'back': 1194,\n",
       " 'gr8': 3218,\n",
       " 'rock': 5815,\n",
       " 'sis': 6190,\n",
       " 'send': 6019,\n",
       " 'wen': 7462,\n",
       " 'gals': 3071,\n",
       " 'anyone': 995,\n",
       " 'down': 2425,\n",
       " 'driving': 2456,\n",
       " 'centre': 1702,\n",
       " 'tmr': 6928,\n",
       " 'divorce': 2361,\n",
       " 'she': 6085,\n",
       " 'dude': 2476,\n",
       " 'parked': 5091,\n",
       " 'sunroof': 6608,\n",
       " 'popped': 5312,\n",
       " 'sux': 6638,\n",
       " 'idea': 3593,\n",
       " 'guess': 3272,\n",
       " 'work': 7601,\n",
       " 'hour': 3517,\n",
       " 're': 5597,\n",
       " 'supposed': 6622,\n",
       " 'since': 6177,\n",
       " 'usual': 7232,\n",
       " 'nobody': 4796,\n",
       " 'any': 992,\n",
       " 'interest': 3706,\n",
       " 'figuring': 2841,\n",
       " 'before': 1299,\n",
       " 'last': 4027,\n",
       " 'second': 5980,\n",
       " 'nice': 4767,\n",
       " 'impressively': 3640,\n",
       " 'sensible': 6027,\n",
       " 'feel': 2805,\n",
       " 'fine': 2860,\n",
       " 'yours': 7725,\n",
       " 'waaaat': 7346,\n",
       " 'lololo': 4192,\n",
       " 'next': 4764,\n",
       " 'midnight': 4476,\n",
       " 'earliest': 2500,\n",
       " 'who': 7502,\n",
       " 'first': 2873,\n",
       " 'created': 2058,\n",
       " 'web': 7429,\n",
       " 'page': 5054,\n",
       " 'asjesus': 1091,\n",
       " 'com': 1899,\n",
       " 'read': 5603,\n",
       " 'wrote': 7638,\n",
       " 'waiting': 7359,\n",
       " 'opinions': 4967,\n",
       " 'friend': 3009,\n",
       " 'later': 4032,\n",
       " 'mins': 4501,\n",
       " 'designation': 2258,\n",
       " 'software': 6292,\n",
       " 'developer': 2274,\n",
       " 'chennai': 1762,\n",
       " 'coming': 1911,\n",
       " 'pongal': 5301,\n",
       " 'news': 4760,\n",
       " 'place': 5231,\n",
       " 'becoz': 1285,\n",
       " 'jan': 3790,\n",
       " 'whn': 7501,\n",
       " 'al': 890,\n",
       " 'post': 5332,\n",
       " 'ofice': 4908,\n",
       " 'holiday': 3473,\n",
       " 'cn': 1863,\n",
       " 'fr': 2975,\n",
       " 'duffer': 2479,\n",
       " 'chk': 1795,\n",
       " 'belovd': 1318,\n",
       " 'ms': 4619,\n",
       " 'dict': 2293,\n",
       " 'part': 5093,\n",
       " 'don': 2401,\n",
       " 'initiate': 3682,\n",
       " 'understand': 7149,\n",
       " 'cute': 2126,\n",
       " 'thought': 6867,\n",
       " 'friendship': 3011,\n",
       " 'necessary': 4721,\n",
       " 'share': 6079,\n",
       " 'secret': 5983,\n",
       " 'close': 1845,\n",
       " 'frnd': 3016,\n",
       " 'watever': 7409,\n",
       " 'shared': 6080,\n",
       " 'true': 7049,\n",
       " 'heard': 3384,\n",
       " 'week': 7444,\n",
       " 'done': 2403,\n",
       " 'luv': 4274,\n",
       " 'ya': 7674,\n",
       " 'wnt': 7576,\n",
       " 'bmw': 1404,\n",
       " 'urgently': 7209,\n",
       " 'vry': 7332,\n",
       " 'hv': 3569,\n",
       " 'shortage': 6120,\n",
       " 'lacs': 3996,\n",
       " 'source': 6348,\n",
       " 'arng': 1068,\n",
       " 'dis': 2335,\n",
       " 'amt': 950,\n",
       " 'thats': 6816,\n",
       " 'prob': 5433,\n",
       " 'message': 4462,\n",
       " 'please': 5253,\n",
       " '08715205273': 119,\n",
       " 'pa': 5047,\n",
       " 'selected': 6006,\n",
       " 'morning': 4591,\n",
       " 'im': 3618,\n",
       " 'suffering': 6581,\n",
       " 'fever': 2821,\n",
       " 'dysentry': 2494,\n",
       " 'able': 746,\n",
       " 'desparate': 2260,\n",
       " 'recorded': 5647,\n",
       " 'left': 4071,\n",
       " 'other': 5004,\n",
       " 'listen': 4148,\n",
       " 'hear': 3383,\n",
       " 'voice': 7322,\n",
       " 'thanks': 6807,\n",
       " 've': 7265,\n",
       " 'lovely': 4237,\n",
       " 'wisheds': 7549,\n",
       " 'okay': 4919,\n",
       " 'same': 5902,\n",
       " 'clarification': 1824,\n",
       " 'mmmmmm': 4537,\n",
       " 'ahmad': 870,\n",
       " 'year': 7688,\n",
       " 'begin': 1302,\n",
       " 'takes': 6691,\n",
       " 'closer': 1848,\n",
       " 'being': 1309,\n",
       " 'side': 6155,\n",
       " 'happy': 3338,\n",
       " 'sometimes': 6311,\n",
       " 'heart': 3387,\n",
       " 'remembrs': 5699,\n",
       " 'someone': 6304,\n",
       " 'forgets': 2949,\n",
       " 'soon': 6325,\n",
       " 'bcoz': 1267,\n",
       " 'everyone': 2672,\n",
       " 'liked': 4118,\n",
       " 'ones': 4944,\n",
       " 'remembered': 5696,\n",
       " 'everytime': 2677,\n",
       " 'bslvyl': 1529,\n",
       " 'aren': 1050,\n",
       " 'between': 1335,\n",
       " 'class': 1828,\n",
       " 'need': 4726,\n",
       " 'shower': 6134,\n",
       " 'chikku': 1778,\n",
       " 'room': 5829,\n",
       " 'nw': 4866,\n",
       " 'bus': 1564,\n",
       " '85233': 667,\n",
       " 'ringtone': 5799,\n",
       " 'real': 5607,\n",
       " 'his': 3444,\n",
       " 'tomorrow': 6961,\n",
       " 'nah': 4679,\n",
       " 'help': 3407,\n",
       " 'never': 4753,\n",
       " 'had': 3295,\n",
       " 'iphone': 3736,\n",
       " 'too': 6973,\n",
       " 'watching': 7406,\n",
       " 'surya': 6636,\n",
       " 'movie': 4610,\n",
       " 'vijay': 7297,\n",
       " 'pokkiri': 5290,\n",
       " 'sure': 6626,\n",
       " 'knows': 3968,\n",
       " 'ain': 876,\n",
       " 'smokin': 6262,\n",
       " 'feathery': 2799,\n",
       " 'bowa': 1446,\n",
       " 'guys': 3286,\n",
       " 'type': 7109,\n",
       " 'stuff': 6544,\n",
       " 'sing': 6180,\n",
       " 'welcome': 7458,\n",
       " 'age': 855,\n",
       " 'gender': 3110,\n",
       " '24m': 356,\n",
       " 'arun': 1082,\n",
       " 'transfr': 7024,\n",
       " 'problem': 5435,\n",
       " 'walk': 7364,\n",
       " 'julianaland': 3870,\n",
       " 'oblivious': 4883,\n",
       " 'things': 6846,\n",
       " 'constantly': 1973,\n",
       " 'ear': 2498,\n",
       " 'while': 7497,\n",
       " 'off': 4896,\n",
       " 'doing': 2392,\n",
       " 'whatever': 7482,\n",
       " 'upset': 7200,\n",
       " 'surprised': 6633,\n",
       " 'mad': 4308,\n",
       " 'finished': 2864,\n",
       " 'reg': 5668,\n",
       " 'pract': 5359,\n",
       " 'lessons': 4089,\n",
       " 'flung': 2912,\n",
       " 'haha': 3298,\n",
       " 'avatar': 1155,\n",
       " 'subtoitles': 6568,\n",
       " 'were': 7467,\n",
       " 'makin': 4337,\n",
       " 'those': 6863,\n",
       " 'weirdy': 7456,\n",
       " 'brownies': 1522,\n",
       " 'sister': 6191,\n",
       " 'awesome': 1170,\n",
       " 'cookies': 1994,\n",
       " 'took': 6974,\n",
       " 'pics': 5210,\n",
       " 'yup': 7740,\n",
       " 'wun': 7649,\n",
       " 'believe': 1310,\n",
       " 'really': 5616,\n",
       " 'neva': 4751,\n",
       " 'msg': 4620,\n",
       " 'shuhui': 6147,\n",
       " 'running': 5865,\n",
       " 'managed': 4345,\n",
       " 'needed': 4728,\n",
       " 'oxygen': 5043,\n",
       " 'resort': 5750,\n",
       " 'roller': 5825,\n",
       " 'option': 4977,\n",
       " 'da': 2136,\n",
       " 'thangam': 6805,\n",
       " 'also': 922,\n",
       " 'goldviking': 3184,\n",
       " '29': 371,\n",
       " 'inviting': 3726,\n",
       " '762': 606,\n",
       " 'sms': 6265,\n",
       " 'ac': 760,\n",
       " '62468': 570,\n",
       " 'awake': 1166,\n",
       " 'fact': 2745,\n",
       " 'cleaning': 1834,\n",
       " 'shows': 6140,\n",
       " 'priority': 5424,\n",
       " 'forwarding': 2967,\n",
       " 'proof': 5465,\n",
       " 'fb': 2797,\n",
       " 'list': 4146,\n",
       " 'yavnt': 7683,\n",
       " 'tried': 7041,\n",
       " 'played': 5245,\n",
       " 'original': 4996,\n",
       " 'either': 2550,\n",
       " 'put': 5514,\n",
       " 'excellent': 2693,\n",
       " 'sub': 6553,\n",
       " 'face': 2742,\n",
       " 'reminder': 5702,\n",
       " 'o2': 4874,\n",
       " '50': 527,\n",
       " 'pounds': 5345,\n",
       " 'credit': 2060,\n",
       " 'details': 2269,\n",
       " 'great': 3240,\n",
       " 'offers': 4902,\n",
       " 'name': 4685,\n",
       " 'house': 3520,\n",
       " 'postcode': 5335,\n",
       " 'best': 1326,\n",
       " 'exam': 2691,\n",
       " 'opinion': 4966,\n",
       " 'jada': 3784,\n",
       " 'kusruthi': 3984,\n",
       " 'lovable': 4233,\n",
       " 'silent': 6167,\n",
       " 'spl': 6394,\n",
       " 'character': 1725,\n",
       " 'matured': 4396,\n",
       " 'stylish': 6552,\n",
       " 'simple': 6173,\n",
       " 'drop': 2459,\n",
       " 'joined': 3841,\n",
       " 'league': 4059,\n",
       " 'people': 5151,\n",
       " 'dont': 2405,\n",
       " 'keep': 3912,\n",
       " 'touch': 6999,\n",
       " 'mean': 4414,\n",
       " 'deal': 2182,\n",
       " 'times': 6902,\n",
       " 'even': 2661,\n",
       " 'personal': 5172,\n",
       " 'cost': 2012,\n",
       " 'talking': 6700,\n",
       " 'bout': 1445,\n",
       " 'almost': 915,\n",
       " 'noon': 4811,\n",
       " 'think': 6847,\n",
       " 'syllabus': 6665,\n",
       " 'account': 775,\n",
       " 'hip': 3442,\n",
       " 'hop': 3492,\n",
       " 'open': 4959,\n",
       " 'jazz': 3804,\n",
       " 'zoom': 7751,\n",
       " 'cine': 1814,\n",
       " 'leh': 4076,\n",
       " 'kb': 3909,\n",
       " 'lesson': 4088,\n",
       " 'safe': 5886,\n",
       " 'trip': 7042,\n",
       " 'nigeria': 4772,\n",
       " 'wish': 7548,\n",
       " 'company': 1920,\n",
       " 'moments': 4568,\n",
       " 'pizza': 5230,\n",
       " 'if': 3604,\n",
       " 'without': 7559,\n",
       " 'could': 2022,\n",
       " 'big': 1345,\n",
       " 'sale': 5893,\n",
       " 'together': 6944,\n",
       " 'aight': 875,\n",
       " 'happening': 3333,\n",
       " 'polyphonic': 5299,\n",
       " 'tone': 6963,\n",
       " 'pt2': 5488,\n",
       " '87575': 683,\n",
       " '1st': 325,\n",
       " 'txtin': 7100,\n",
       " '150p': 295,\n",
       " 'hl': 3452,\n",
       " '4info': 509,\n",
       " 'beautiful': 1279,\n",
       " 'hunt': 3559,\n",
       " 'waste': 7399,\n",
       " 'wonderful': 7589,\n",
       " 'nordstrom': 4817,\n",
       " 'haf': 3297,\n",
       " 'busy': 1568,\n",
       " 'norm': 4818,\n",
       " '15': 292,\n",
       " 'st': 6430,\n",
       " 'tests': 6786,\n",
       " 'sort': 6336,\n",
       " 'library': 4100,\n",
       " 'point': 5286,\n",
       " 'tomo': 6959,\n",
       " 'access': 765,\n",
       " 'til': 6898,\n",
       " 'end': 2582,\n",
       " 'march': 4360,\n",
       " 'better': 1332,\n",
       " 'probably': 5434,\n",
       " 'argh': 1056,\n",
       " 'fuck': 3031,\n",
       " 'team': 6739,\n",
       " 'india': 3667,\n",
       " 'these': 6836,\n",
       " 'ideas': 3595,\n",
       " 'few': 2822,\n",
       " 'game': 3074,\n",
       " 'mall': 4342,\n",
       " 'iouri': 3730,\n",
       " 'kaila': 3889,\n",
       " 'loan': 4170,\n",
       " 'purpose': 5508,\n",
       " '500': 528,\n",
       " '75': 602,\n",
       " '000': 1,\n",
       " 'homeowners': 3478,\n",
       " 'tenants': 6768,\n",
       " 'previously': 5411,\n",
       " 'refused': 5667,\n",
       " '0800': 45,\n",
       " '1956669': 316,\n",
       " 'dunno': 2484,\n",
       " 'eatin': 2515,\n",
       " 'frens': 2999,\n",
       " 'wan': 7376,\n",
       " 'eat': 2513,\n",
       " 'lar': 4021,\n",
       " '08702490080': 75,\n",
       " 'tells': 6759,\n",
       " '09066358152': 216,\n",
       " '5000': 529,\n",
       " 'enter': 2606,\n",
       " 'prompts': 5463,\n",
       " 'careful': 1651,\n",
       " 'ah': 865,\n",
       " 'confuses': 1956,\n",
       " 'doesn': 2381,\n",
       " 'wanna': 7379,\n",
       " 'spend': 6381,\n",
       " 'would': 7622,\n",
       " 'vewy': 7281,\n",
       " 'lubly': 4261,\n",
       " 'xxx': 7666,\n",
       " 'alive': 908,\n",
       " 'correct': 2006,\n",
       " 'figure': 2839,\n",
       " 'itself': 3767,\n",
       " 'try': 7057,\n",
       " 'warned': 7390,\n",
       " 'sprint': 6420,\n",
       " 'slow': 6238,\n",
       " 'prolly': 5454,\n",
       " 'tv': 7083,\n",
       " 'no1': 4794,\n",
       " 'nokia': 4802,\n",
       " '8007': 625,\n",
       " 'txting': 7101,\n",
       " 'mates': 4386,\n",
       " 'getzed': 3132,\n",
       " 'co': 1866,\n",
       " 'uk': 7123,\n",
       " 'pobox': 5270,\n",
       " '36504': 442,\n",
       " 'w45wq': 7343,\n",
       " 'norm150p': 4819,\n",
       " 'double': 2420,\n",
       " 'eviction': 2680,\n",
       " 'spiral': 6389,\n",
       " 'michael': 4473,\n",
       " 'riddance': 5787,\n",
       " 'black': 1371,\n",
       " 'shirt': 6105,\n",
       " 'blue': 1400,\n",
       " 'jeans': 3809,\n",
       " 'thk': 6856,\n",
       " 'mostly': 4596,\n",
       " 'sports': 6413,\n",
       " 'lyk': 4283,\n",
       " 'footbl': 2939,\n",
       " 'crckt': 2056,\n",
       " 'schedule': 5950,\n",
       " 'weekend': 7446,\n",
       " 'stand': 6441,\n",
       " 'away': 1169,\n",
       " 'ache': 780,\n",
       " 'wonder': 7588,\n",
       " 'crave': 2052,\n",
       " 'ten': 6767,\n",
       " 'rs': 5847,\n",
       " 'shelf': 6089,\n",
       " 'two': 7093,\n",
       " 'egg': 2541,\n",
       " 'madam': 4311,\n",
       " 'increase': 3659,\n",
       " 'winning': 7540,\n",
       " 'continued': 1982,\n",
       " 'support': 6620,\n",
       " 'in2': 3645,\n",
       " 'draw': 2440,\n",
       " '100': 246,\n",
       " 'us': 7218,\n",
       " 'president': 5398,\n",
       " 'ans': 980,\n",
       " '80082': 626,\n",
       " 'blimey': 1385,\n",
       " 'exercise': 2703,\n",
       " 'yeah': 7687,\n",
       " 'kinda': 3944,\n",
       " 'wot': 7618,\n",
       " 'stupid': 6548,\n",
       " 'always': 927,\n",
       " 'sending': 6021,\n",
       " 'pandy': 5067,\n",
       " 'mental': 4451,\n",
       " 'sry': 6429,\n",
       " 'parents': 5087,\n",
       " 'enc': 2581,\n",
       " 'nyc': 4869,\n",
       " 'audiitions': 1139,\n",
       " 'trying': 7058,\n",
       " 'relocate': 5689,\n",
       " 'came': 1618,\n",
       " 'hostel': 3510,\n",
       " 'near': 4716,\n",
       " 'mca': 4407,\n",
       " 'conform': 1954,\n",
       " 'mayb': 4403,\n",
       " 'rite': 5807,\n",
       " 'feeling': 2807,\n",
       " 'gd': 3103,\n",
       " 'faster': 2782,\n",
       " 'group': 3256,\n",
       " 'attached': 1125,\n",
       " 'liao': 4097,\n",
       " 'monkey': 4575,\n",
       " 'bitch': 1366,\n",
       " 'asshole': 1104,\n",
       " 'winner': 7538,\n",
       " 'network': 4748,\n",
       " 'hvae': 3571,\n",
       " '900': 709,\n",
       " 'collect': 1889,\n",
       " '09061701444': 182,\n",
       " '24': 354,\n",
       " 'acl03530150pm': 784,\n",
       " 'bsn': 1530,\n",
       " 'advising': 830,\n",
       " '18': 311,\n",
       " 'days': 2176,\n",
       " 'euro2004': 2654,\n",
       " 'kickoff': 3931,\n",
       " 'kept': 3918,\n",
       " 'informed': 3679,\n",
       " 'latest': 4033,\n",
       " 'results': 5763,\n",
       " 'daily': 2143,\n",
       " 'unsubscribe': 7182,\n",
       " 'euro': 2653,\n",
       " '83222': 651,\n",
       " 'passable': 5104,\n",
       " 'high': 3434,\n",
       " 'score': 5957,\n",
       " 'phd': 5187,\n",
       " '5years': 561,\n",
       " 'salary': 5892,\n",
       " ...}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_np = X_train_cv.toarray()\n",
    "X_train_np[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 786,  846,  958, 1048, 1185, 3021, 3190, 3362, 3493, 3525, 3714,\n",
       "        3952, 4234, 4404, 4513, 4667, 5107, 5974, 6300, 6817, 6940, 7360,\n",
       "        7715, 7721]),)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(X_train_np[0]!=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Good afternon, my love. How are today? I hope your good and maybe have some interviews. I wake and miss you babe. A passionate kiss from across the sea'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:4][3377]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_np[0][3377]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the naive bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_cv,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_cv=v.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       963\n",
      "           1       0.98      0.95      0.97       152\n",
      "\n",
      "    accuracy                           0.99      1115\n",
      "   macro avg       0.99      0.98      0.98      1115\n",
      "weighted avg       0.99      0.99      0.99      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = model.predict(X_test_cv)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails = [\n",
    "    'Hey mohan, can we get together to watch footbal game tomorrow?',\n",
    "    'Upto 20% discount on parking, exclusive offer just for you. Dont miss this reward!'\n",
    "]\n",
    "\n",
    "emails_count = v.transform(emails)\n",
    "model.predict(emails_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model using sklearn pipeline and reduce number of lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "clf=Pipeline([\n",
    "    ('vectorizer',CountVectorizer()),\n",
    "    ('nb',MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;, CountVectorizer()), (&#x27;nb&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;, CountVectorizer()), (&#x27;nb&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vectorizer', CountVectorizer()), ('nb', MultinomialNB())])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       963\n",
      "           1       0.98      0.95      0.97       152\n",
      "\n",
      "    accuracy                           0.99      1115\n",
      "   macro avg       0.99      0.98      0.98      1115\n",
      "weighted avg       0.99      0.99      0.99      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words: Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I first saw Jake Gyllenhaal in Jarhead (2005) ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I enjoyed the movie and the story immensely! I...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I had a hard time sitting through this. Every ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's hard to imagine that anyone could find th...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is one military drama I like a lot! Tom B...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  I first saw Jake Gyllenhaal in Jarhead (2005) ...  positive\n",
       "1  I enjoyed the movie and the story immensely! I...  positive\n",
       "2  I had a hard time sitting through this. Every ...  negative\n",
       "3  It's hard to imagine that anyone could find th...  negative\n",
       "4  This is one military drama I like a lot! Tom B...  positive"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. read the data provided in the same directory with name 'movies_sentiment_data.csv' and store it in df variable\n",
    "df = pd.read_csv(\"movies_sentiment_data.csv\")\n",
    "\n",
    "\n",
    "#2. print the shape of the data\n",
    "print(df.shape)\n",
    "\n",
    "#3. print top 5 datapoints\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new column \"Category\" which represent 1 if the sentiment is positive or 0 if it is negative\n",
    "df['Category'] = df['sentiment'].apply(lambda x: 1 if x =='positive' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    9500\n",
       "0    9500\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the distribution of 'Category' and see whether the Target labels are balanced or not.\n",
    "\n",
    "df['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the 'train-test' splitting with test size of 20%\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.review, df.Category, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.82      0.83      1897\n",
      "           1       0.83      0.83      0.83      1903\n",
      "\n",
      "    accuracy                           0.83      3800\n",
      "   macro avg       0.83      0.83      0.83      3800\n",
      "weighted avg       0.83      0.83      0.83      3800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1. create a pipeline object\n",
    "clf = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),                                                    #initializing the vectorizer\n",
    "    ('random_forest', (RandomForestClassifier(n_estimators=50, criterion='entropy')))      #using the RandomForest classifier\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.65      0.64      1897\n",
      "           1       0.64      0.63      0.63      1903\n",
      "\n",
      "    accuracy                           0.64      3800\n",
      "   macro avg       0.64      0.64      0.64      3800\n",
      "weighted avg       0.64      0.64      0.64      3800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1. create a pipeline object\n",
    "clf = Pipeline([\n",
    "                \n",
    "     ('vectorizer', CountVectorizer()),   \n",
    "      ('KNN', (KNeighborsClassifier(n_neighbors=10, metric = 'euclidean')))   #using the KNN classifier with 10 neighbors \n",
    "])\n",
    "\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.87      0.85      1897\n",
      "           1       0.87      0.83      0.85      1903\n",
      "\n",
      "    accuracy                           0.85      3800\n",
      "   macro avg       0.85      0.85      0.85      3800\n",
      "weighted avg       0.85      0.85      0.85      3800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1. create a pipeline object\n",
    "clf = Pipeline([\n",
    "                \n",
    "     ('vectorizer', CountVectorizer()),   \n",
    "      ('Multi NB', MultinomialNB())   #using the Multinomial Naive Bayes classifier \n",
    "])\n",
    "\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of N - Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thor': 5, 'hathodawala': 1, 'is': 2, 'looking': 4, 'for': 0, 'job': 3}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "v=CountVectorizer()\n",
    "v.fit([\"Thor Hathodawala is looking for a job\"])\n",
    "v.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thor': 12,\n",
       " 'hathodawala': 2,\n",
       " 'is': 5,\n",
       " 'looking': 9,\n",
       " 'for': 0,\n",
       " 'job': 8,\n",
       " 'thor hathodawala': 13,\n",
       " 'hathodawala is': 3,\n",
       " 'is looking': 6,\n",
       " 'looking for': 10,\n",
       " 'for job': 1,\n",
       " 'thor hathodawala is': 14,\n",
       " 'hathodawala is looking': 4,\n",
       " 'is looking for': 7,\n",
       " 'looking for job': 11}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v=CountVectorizer(ngram_range=(1,3))\n",
    "v.fit([\"Thor Hathodawala is looking for a job\"])\n",
    "v.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "def preprocess(text):\n",
    "    doc=nlp(text)\n",
    "    filtered_tokens=[]\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        filtered_tokens.append(token.lemma_)\n",
    "    return \" \".join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thor eat pizza'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(\"Thor ate pizza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thor eat pizza', 'Loki tall', 'Loki eat pizza']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"Thor ate pizza\",\n",
    "    \"Loki is tall\",\n",
    "    \"Loki is eating pizza\"\n",
    "]\n",
    "corpus_processed=[\n",
    "    preprocess(text) for text in corpus\n",
    "]\n",
    "corpus_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thor': 7,\n",
       " 'eat': 0,\n",
       " 'pizza': 5,\n",
       " 'thor eat': 8,\n",
       " 'eat pizza': 1,\n",
       " 'loki': 2,\n",
       " 'tall': 6,\n",
       " 'loki tall': 4,\n",
       " 'loki eat': 3}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = CountVectorizer(ngram_range=(1,2))\n",
    "v.fit(corpus_processed)\n",
    "v.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 1, 0, 1, 1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.transform([\"Thor eat pizza\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.transform([\"Hulk eat pizza\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12695, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Watching Schrödinger's Cat Die University of C...</td>\n",
       "      <td>SCIENCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WATCH: Freaky Vortex Opens Up In Flooded Lake</td>\n",
       "      <td>SCIENCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Entrepreneurs Today Don't Need a Big Budget to...</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>These Roads Could Recharge Your Electric Car A...</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Civilian 'Guard' Fires Gun While 'Protecting' ...</td>\n",
       "      <td>CRIME</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  category\n",
       "0  Watching Schrödinger's Cat Die University of C...   SCIENCE\n",
       "1     WATCH: Freaky Vortex Opens Up In Flooded Lake    SCIENCE\n",
       "2  Entrepreneurs Today Don't Need a Big Budget to...  BUSINESS\n",
       "3  These Roads Could Recharge Your Electric Car A...  BUSINESS\n",
       "4  Civilian 'Guard' Fires Gun While 'Protecting' ...     CRIME"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json('news_dataset.json')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BUSINESS    4254\n",
       "SPORTS      4167\n",
       "CRIME       2893\n",
       "SCIENCE     1381\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle Imbalance datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples=1381\n",
    "df_business=df[df.category==\"BUSINESS\"].sample(min_samples,random_state=1234)\n",
    "df_sports=df[df.category==\"SPORTS\"].sample(min_samples,random_state=1234)\n",
    "df_crime=df[df.category==\"CRIME\"].sample(min_samples,random_state=1234)\n",
    "df_science=df[df.category==\"SCIENCE\"].sample(min_samples,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced=pd.concat([df_business,df_sports,df_crime,df_science],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BUSINESS    1381\n",
       "SPORTS      1381\n",
       "CRIME       1381\n",
       "SCIENCE     1381\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balanced.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert text category to a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = {'BUSINESS': 0, 'SPORTS': 1, 'CRIME': 2, 'SCIENCE': 3}\n",
    "\n",
    "df_balanced['category_num'] = df_balanced['category'].map({\n",
    "    'BUSINESS': 0,\n",
    "    'SPORTS': 1, \n",
    "    'CRIME': 2, \n",
    "    'SCIENCE': 3\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9196</th>\n",
       "      <td>Krugman: We Should 'Soak The Rich'</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3111</th>\n",
       "      <td>Krugman: Why America Is Still Stuck</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10567</th>\n",
       "      <td>Bay Area School Gets Rich Quick On Snapchat In...</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5575</th>\n",
       "      <td>Is It Safe To Eat At Chipotle After The E. Col...</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>Leaked Document Shows Strong Business Support ...</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  category  \\\n",
       "9196                 Krugman: We Should 'Soak The Rich'   BUSINESS   \n",
       "3111                Krugman: Why America Is Still Stuck   BUSINESS   \n",
       "10567  Bay Area School Gets Rich Quick On Snapchat In...  BUSINESS   \n",
       "5575   Is It Safe To Eat At Chipotle After The E. Col...  BUSINESS   \n",
       "1332   Leaked Document Shows Strong Business Support ...  BUSINESS   \n",
       "\n",
       "       category_num  \n",
       "9196              0  \n",
       "3111              0  \n",
       "10567             0  \n",
       "5575              0  \n",
       "1332              0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balanced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a model with original text (no pre processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_balanced.text, \n",
    "    df_balanced.category_num, \n",
    "    test_size=0.2, # 20% samples will go to test dataset\n",
    "    random_state=2022,\n",
    "    stratify=df_balanced.category_num\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4419,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1853     WATCH: Strange Way Jumbo Squid 'Talk' To Each ...\n",
       "5371     Gorgeous New NASA Image Shows Earth 'Rising' O...\n",
       "3883     Medics On Football Sidelines Must Have The For...\n",
       "10416    Teen Made Up Clown Attack To Avoid Being Fired...\n",
       "11079    This Man Faces Life in Prison for ... Rapping ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    1105\n",
       "2    1105\n",
       "0    1105\n",
       "1    1104\n",
       "Name: category_num, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    277\n",
       "0    276\n",
       "3    276\n",
       "2    276\n",
       "Name: category_num, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 1 : Use 1-gram which is nothing but a Bag Of Words (BOW) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.92      0.83       276\n",
      "           1       0.92      0.83      0.87       277\n",
      "           2       0.87      0.87      0.87       276\n",
      "           3       0.89      0.80      0.84       276\n",
      "\n",
      "    accuracy                           0.86      1105\n",
      "   macro avg       0.86      0.86      0.86      1105\n",
      "weighted avg       0.86      0.86      0.86      1105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#1. create a pipeline object\n",
    "clf = Pipeline([\n",
    "     ('vectorizer_bow', CountVectorizer(ngram_range = (1, 1))),        #using the ngram_range parameter \n",
    "     ('Multi NB', MultinomialNB())         \n",
    "])\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10987    Amazon HQ2 In The Time Of Climate Change Ranki...\n",
       "1527     High-Speed Camera Captures Amazing Downward An...\n",
       "10152    7 Keys To A Happy, Healthy Brain Take care of ...\n",
       "1423     'All You Americans Are Fired' The H-2 guest wo...\n",
       "11883    The Real Prejudice Behind the Garner/Brown Dec...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 0, 0, 0])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10987    0\n",
       "1527     3\n",
       "10152    3\n",
       "1423     0\n",
       "11883    2\n",
       "Name: category_num, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 2 : Use 1-gram and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.94      0.80       276\n",
      "           1       0.93      0.78      0.85       277\n",
      "           2       0.88      0.87      0.88       276\n",
      "           3       0.91      0.75      0.82       276\n",
      "\n",
      "    accuracy                           0.83      1105\n",
      "   macro avg       0.85      0.83      0.84      1105\n",
      "weighted avg       0.85      0.83      0.84      1105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vectorizer_1_2_gram', CountVectorizer(ngram_range = (1, 2))),        #using the ngram_range parameter \n",
    "     ('Multi NB', MultinomialNB())         \n",
    "])\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 3 : Use 1-gram to trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.93      0.78       276\n",
      "           1       0.92      0.78      0.84       277\n",
      "           2       0.88      0.86      0.87       276\n",
      "           3       0.91      0.73      0.81       276\n",
      "\n",
      "    accuracy                           0.82      1105\n",
      "   macro avg       0.85      0.82      0.83      1105\n",
      "weighted avg       0.85      0.82      0.83      1105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1. create a pipeline object\n",
    "clf = Pipeline([\n",
    "    ('vectorizer_1_3_grams', CountVectorizer(ngram_range = (1, 3))),        #using the ngram_range parameter \n",
    "     ('Multi NB', MultinomialNB())         \n",
    "])\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use text pre-processing to remove stop words, punctuations and apply lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced['preprocessed_txt'] = df_balanced['text'].apply(preprocess) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_num</th>\n",
       "      <th>preprocessed_txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9196</th>\n",
       "      <td>Krugman: We Should 'Soak The Rich'</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>0</td>\n",
       "      <td>krugman soak Rich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3111</th>\n",
       "      <td>Krugman: Why America Is Still Stuck</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>0</td>\n",
       "      <td>krugman America stuck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10567</th>\n",
       "      <td>Bay Area School Gets Rich Quick On Snapchat In...</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>0</td>\n",
       "      <td>Bay Area School get Rich Quick Snapchat Invest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5575</th>\n",
       "      <td>Is It Safe To Eat At Chipotle After The E. Col...</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>0</td>\n",
       "      <td>safe eat Chipotle E. Coli Outbreak recent news...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>Leaked Document Shows Strong Business Support ...</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>0</td>\n",
       "      <td>Leaked Document show Strong Business Support r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  category  \\\n",
       "9196                 Krugman: We Should 'Soak The Rich'   BUSINESS   \n",
       "3111                Krugman: Why America Is Still Stuck   BUSINESS   \n",
       "10567  Bay Area School Gets Rich Quick On Snapchat In...  BUSINESS   \n",
       "5575   Is It Safe To Eat At Chipotle After The E. Col...  BUSINESS   \n",
       "1332   Leaked Document Shows Strong Business Support ...  BUSINESS   \n",
       "\n",
       "       category_num                                   preprocessed_txt  \n",
       "9196              0                                  krugman soak Rich  \n",
       "3111              0                              krugman America stuck  \n",
       "10567             0  Bay Area School get Rich Quick Snapchat Invest...  \n",
       "5575              0  safe eat Chipotle E. Coli Outbreak recent news...  \n",
       "1332              0  Leaked Document show Strong Business Support r...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balanced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a model with pre processed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_balanced.preprocessed_txt, \n",
    "    df_balanced.category_num, \n",
    "    test_size=0.2, # 20% samples will go to test dataset\n",
    "    random_state=2022,\n",
    "    stratify=df_balanced.category_num\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4419,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1853                    watch strange Way Jumbo Squid talk\n",
       "5371     gorgeous New NASA Image show Earth rise Moon r...\n",
       "3883     medic Football Sidelines fortitude throw Towel...\n",
       "10416    Teen Clown Attack avoid fire Lateness Police c...\n",
       "11079    Man face Life Prison rap hear free speech free...\n",
       "Name: preprocessed_txt, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    1105\n",
       "2    1105\n",
       "0    1105\n",
       "1    1104\n",
       "Name: category_num, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    277\n",
       "0    276\n",
       "3    276\n",
       "2    276\n",
       "Name: category_num, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.92      0.86       276\n",
      "           1       0.94      0.85      0.90       277\n",
      "           2       0.88      0.92      0.90       276\n",
      "           3       0.91      0.83      0.87       276\n",
      "\n",
      "    accuracy                           0.88      1105\n",
      "   macro avg       0.89      0.88      0.88      1105\n",
      "weighted avg       0.89      0.88      0.88      1105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1. create a pipeline object\n",
    "clf = Pipeline([\n",
    "    ('vectorizer_bow', CountVectorizer(ngram_range = (1, 2))),        #using the ngram_range parameter \n",
    "    ('Multi NB', MultinomialNB())\n",
    "])\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[254,   5,   5,  12],\n",
       "       [ 14, 236,  21,   6],\n",
       "       [ 12,   4, 255,   5],\n",
       "       [ 35,   5,   8, 228]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(69.0, 0.5, 'Truth')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of n_grams: Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9900, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top Trump Surrogate BRUTALLY Stabs Him In The...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. conservative leader optimistic of common ...</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trump proposes U.S. tax overhaul, stirs concer...</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Court Forces Ohio To Allow Millions Of Illega...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Democrats say Trump agrees to work on immigrat...</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text label\n",
       "0   Top Trump Surrogate BRUTALLY Stabs Him In The...  Fake\n",
       "1  U.S. conservative leader optimistic of common ...  Real\n",
       "2  Trump proposes U.S. tax overhaul, stirs concer...  Real\n",
       "3   Court Forces Ohio To Allow Millions Of Illega...  Fake\n",
       "4  Democrats say Trump agrees to work on immigrat...  Real"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as ps\n",
    "df=pd.read_csv(\"Fake_Real_Data.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fake    5000\n",
       "Real    4900\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label_num\"]=df['label'].map({\n",
    "    'Fake':0,\n",
    "    'Real':1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top Trump Surrogate BRUTALLY Stabs Him In The...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. conservative leader optimistic of common ...</td>\n",
       "      <td>Real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trump proposes U.S. tax overhaul, stirs concer...</td>\n",
       "      <td>Real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Court Forces Ohio To Allow Millions Of Illega...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Democrats say Trump agrees to work on immigrat...</td>\n",
       "      <td>Real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text label  label_num\n",
       "0   Top Trump Surrogate BRUTALLY Stabs Him In The...  Fake          0\n",
       "1  U.S. conservative leader optimistic of common ...  Real          1\n",
       "2  Trump proposes U.S. tax overhaul, stirs concer...  Real          1\n",
       "3   Court Forces Ohio To Allow Millions Of Illega...  Fake          0\n",
       "4  Democrats say Trump agrees to work on immigrat...  Real          1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling without Pre-processing Text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(df.Text,df.label_num,test_size=0.2,random_state=20222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Shape : (7920,)\n",
      "X_test Shape : (1980,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train Shape :\",X_train.shape)\n",
    "print(\"X_test Shape :\",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       987\n",
      "           1       0.99      0.99      0.99       993\n",
      "\n",
      "    accuracy                           0.99      1980\n",
      "   macro avg       0.99      0.99      0.99      1980\n",
      "weighted avg       0.99      0.99      0.99      1980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf=Pipeline([\n",
    "    ('vectorizer',CountVectorizer(ngram_range=(1,10))),\n",
    "    ('Multi NB',MultinomialNB())\n",
    "])\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use text pre-processing to remove stop words, punctuations and apply lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# load english language model and create nlp object from it\n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "\n",
    "def preprocess(text):\n",
    "    # remove stop words and lemmatize the text\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        filtered_tokens.append(token.lemma_)\n",
    "    \n",
    "    return \" \".join(filtered_tokens) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"preprocessed_txt\"]=df[\"Text\"].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_num</th>\n",
       "      <th>preprocessed_txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top Trump Surrogate BRUTALLY Stabs Him In The...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>0</td>\n",
       "      <td>Trump Surrogate BRUTALLY stab pathetic video...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. conservative leader optimistic of common ...</td>\n",
       "      <td>Real</td>\n",
       "      <td>1</td>\n",
       "      <td>U.S. conservative leader optimistic common gro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trump proposes U.S. tax overhaul, stirs concer...</td>\n",
       "      <td>Real</td>\n",
       "      <td>1</td>\n",
       "      <td>trump propose U.S. tax overhaul stir concern d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Court Forces Ohio To Allow Millions Of Illega...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>0</td>\n",
       "      <td>Court Forces Ohio allow Millions illegally p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Democrats say Trump agrees to work on immigrat...</td>\n",
       "      <td>Real</td>\n",
       "      <td>1</td>\n",
       "      <td>democrat trump agree work immigration bill wal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text label  label_num  \\\n",
       "0   Top Trump Surrogate BRUTALLY Stabs Him In The...  Fake          0   \n",
       "1  U.S. conservative leader optimistic of common ...  Real          1   \n",
       "2  Trump proposes U.S. tax overhaul, stirs concer...  Real          1   \n",
       "3   Court Forces Ohio To Allow Millions Of Illega...  Fake          0   \n",
       "4  Democrats say Trump agrees to work on immigrat...  Real          1   \n",
       "\n",
       "                                    preprocessed_txt  \n",
       "0    Trump Surrogate BRUTALLY stab pathetic video...  \n",
       "1  U.S. conservative leader optimistic common gro...  \n",
       "2  trump propose U.S. tax overhaul stir concern d...  \n",
       "3    Court Forces Ohio allow Millions illegally p...  \n",
       "4  democrat trump agree work immigration bill wal...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       987\n",
      "           1       0.98      0.99      0.99       993\n",
      "\n",
      "    accuracy                           0.99      1980\n",
      "   macro avg       0.99      0.99      0.99      1980\n",
      "weighted avg       0.99      0.99      0.99      1980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "X_train,X_test,y_train,y_test=train_test_split(df.preprocessed_txt,df.label_num,test_size=0.2,random_state=20222)\n",
    "clf=Pipeline([\n",
    "    ('vectorizer',CountVectorizer(ngram_range=(1,10))),\n",
    "    ('Multi NB',MultinomialNB())\n",
    "])\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
